#!/usr/bin/env python3
"""
æµ‹è¯•arXivè¿æ¥ - ä¸éœ€è¦OpenAI APIå¯†é’¥

è¿™ä¸ªè„šæœ¬åªæµ‹è¯•arXiv APIè¿æ¥å’Œè®ºæ–‡æŠ“å–åŠŸèƒ½ï¼Œä¸æ¶‰åŠGPTè¿‡æ»¤ã€‚
"""

import requests
import feedparser
from datetime import datetime, timezone, timedelta

def test_arxiv_connection():
    """æµ‹è¯•arXiv APIè¿æ¥"""
    print("ğŸ” æµ‹è¯•arXiv APIè¿æ¥...")
    
    try:
        # æµ‹è¯•æœ€åŸºæœ¬çš„arXivæŸ¥è¯¢
        url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": "cat:cs.AI",
            "sortBy": "submittedDate", 
            "sortOrder": "descending",
            "max_results": 10
        }
        
        print(f"ğŸ“¡ å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"ğŸ“‹ æŸ¥è¯¢å‚æ•°: {params}")
        
        response = requests.get(url, params=params, timeout=15)
        print(f"âœ… HTTPçŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            entries = feed.entries
            print(f"ğŸ“„ è·å–åˆ° {len(entries)} ç¯‡è®ºæ–‡")
            
            if entries:
                print(f"\nğŸ“ è®ºæ–‡æ ·æœ¬:")
                for i, entry in enumerate(entries[:3], 1):
                    print(f"\n{i}. æ ‡é¢˜: {entry.title}")
                    print(f"   å‘å¸ƒæ—¶é—´: {entry.published}")
                    print(f"   æ›´æ–°æ—¶é—´: {entry.updated}")
                    print(f"   ç±»åˆ«: {[tag.term for tag in entry.tags] if hasattr(entry, 'tags') else 'æ— '}")
                    print(f"   æ‘˜è¦é•¿åº¦: {len(entry.summary)} å­—ç¬¦")
                    print(f"   æ‘˜è¦é¢„è§ˆ: {entry.summary[:150]}...")
                return True
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ arXivè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_date_filtering():
    """æµ‹è¯•æ—¥æœŸè¿‡æ»¤åŠŸèƒ½"""
    print(f"\nğŸ• æµ‹è¯•æ—¥æœŸè¿‡æ»¤åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•æœ€è¿‘3å¤©çš„è®ºæ–‡
        url = "http://export.arxiv.org/api/query"
        
        # æ„å»ºåŒ…å«å¤šä¸ªCSç±»åˆ«çš„æŸ¥è¯¢
        categories = ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE", "cs.RO", "cs.IR", "cs.HC", "stat.ML"]
        category_query = " OR ".join(f"cat:{cat}" for cat in categories)
        
        params = {
            "search_query": f"({category_query})",
            "sortBy": "submittedDate",
            "sortOrder": "descending",
            "max_results": 100
        }
        
        print(f"ğŸ“‹ æœç´¢ç±»åˆ«: {', '.join(categories)}")
        print(f"ğŸ“¦ è¯·æ±‚æœ€å¤š100ç¯‡è®ºæ–‡...")
        
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            entries = feed.entries
            print(f"ğŸ“„ æ€»å…±è·å–: {len(entries)} ç¯‡è®ºæ–‡")
            
            # åˆ†ææ—¥æœŸåˆ†å¸ƒ
            now = datetime.now(timezone.utc)
            cutoff_1day = now - timedelta(days=1)
            cutoff_3days = now - timedelta(days=3)
            cutoff_7days = now - timedelta(days=7)
            
            recent_1day = 0
            recent_3days = 0
            recent_7days = 0
            
            for entry in entries:
                paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                
                if paper_date >= cutoff_1day:
                    recent_1day += 1
                if paper_date >= cutoff_3days:
                    recent_3days += 1
                if paper_date >= cutoff_7days:
                    recent_7days += 1
            
            print(f"\nğŸ“Š æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡:")
            print(f"   - æœ€è¿‘1å¤©: {recent_1day} ç¯‡")
            print(f"   - æœ€è¿‘3å¤©: {recent_3days} ç¯‡")
            print(f"   - æœ€è¿‘7å¤©: {recent_7days} ç¯‡")
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ ç¯‡è®ºæ–‡
            if entries:
                print(f"\nğŸ“ æœ€æ–°è®ºæ–‡æ ·æœ¬:")
                for i, entry in enumerate(entries[:5], 1):
                    paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                    print(f"\n{i}. {entry.title[:80]}...")
                    print(f"   æ›´æ–°æ—¶é—´: {paper_date.strftime('%Y-%m-%d %H:%M')} UTC")
                    print(f"   ç±»åˆ«: {', '.join([tag.term for tag in entry.tags][:3])}")
            
            return True
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ æ—¥æœŸè¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    print("ğŸš€ å¼€å§‹ArXivè¿æ¥æµ‹è¯•...")
    print("=" * 60)
    
    success1 = test_arxiv_connection()
    success2 = test_date_filtering()
    
    print("\n" + "=" * 60)
    if success1 and success2:
        print("âœ… arXivè¿æ¥æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ¯ æµ‹è¯•ç»“æœ:")
        print("   - arXiv APIè¿æ¥æ­£å¸¸")
        print("   - è®ºæ–‡æŠ“å–åŠŸèƒ½æ­£å¸¸")
        print("   - æ—¥æœŸè¿‡æ»¤åŠŸèƒ½æ­£å¸¸")
        print("\nğŸ’¡ æ¥ä¸‹æ¥éœ€è¦:")
        print("   - è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        print("   - è¿è¡Œå®Œæ•´çš„è°ƒè¯•è„šæœ¬: python scripts/debug_fetch.py")
    else:
        print("âŒ æµ‹è¯•å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    
    print("=" * 60)

if __name__ == "__main__":
    main() 