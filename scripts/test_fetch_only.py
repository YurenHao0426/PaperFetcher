#!/usr/bin/env python3
"""
æµ‹è¯•è®ºæ–‡æŠ“å–åŠŸèƒ½ - æ˜¾ç¤ºæ”¹è¿›çš„æ—¥å¿—

è¿™ä¸ªè„šæœ¬åªæµ‹è¯•è®ºæ–‡æŠ“å–éƒ¨åˆ†ï¼Œå±•ç¤ºåˆ†é¡µè¿‡ç¨‹å’Œæ—¥æœŸåˆ†å¸ƒï¼Œä¸éœ€è¦OpenAI APIã€‚
"""

import os
import sys
import logging
from datetime import datetime, timezone, timedelta
from collections import Counter

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Add the parent directory to the path so we can import the main module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.fetch_papers import ArxivPaperFetcher


def test_paper_fetching_with_detailed_logs():
    """æµ‹è¯•è®ºæ–‡æŠ“å–ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„åˆ†é¡µå’Œæ—¥æœŸä¿¡æ¯"""
    
    print("ğŸš€ æµ‹è¯•æ”¹è¿›åçš„è®ºæ–‡æŠ“å–æ—¥å¿—æ˜¾ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„fetcherï¼ˆä¸éœ€è¦OpenAI APIï¼‰
    class MockArxivFetcher:
        def __init__(self):
            import requests
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'PaperFetcher/1.0 (Test)'
            })
        
        def fetch_papers_by_date_range(self, start_date, end_date, max_papers=300):
            """æ¨¡æ‹Ÿæˆ‘ä»¬æ”¹è¿›åçš„æŠ“å–å‡½æ•°"""
            logger.info(f"ğŸ” å¼€å§‹ä»arXivæŠ“å–è®ºæ–‡: {start_date.date()} åˆ° {end_date.date()}")
            logger.info(f"ğŸ“‹ ç›®æ ‡ç±»åˆ«: cs.AI, cs.CL, cs.CV, cs.LG, cs.NE, cs.RO, cs.IR, cs.HC, stat.ML")
            
            from scripts.fetch_papers import ARXIV_BASE_URL, CS_CATEGORIES, MAX_RESULTS_PER_BATCH
            import requests
            import feedparser
            
            # Build category query
            category_query = " OR ".join(f"cat:{cat}" for cat in CS_CATEGORIES)
            
            all_papers = []
            start_index = 0
            batch_count = 0
            total_raw_papers = 0
            
            while len(all_papers) < max_papers:
                try:
                    batch_count += 1
                    search_query = f"({category_query})"
                    
                    params = {
                        "search_query": search_query,
                        "sortBy": "submittedDate",
                        "sortOrder": "descending",
                        "start": start_index,
                        "max_results": min(MAX_RESULTS_PER_BATCH, max_papers - len(all_papers))
                    }
                    
                    logger.info(f"ğŸ“¦ ç¬¬{batch_count}æ‰¹æ¬¡: ä»ç´¢å¼•{start_index}å¼€å§‹æŠ“å–...")
                    
                    response = self.session.get(ARXIV_BASE_URL, params=params, timeout=30)
                    response.raise_for_status()
                    
                    feed = feedparser.parse(response.content)
                    entries = feed.entries
                    total_raw_papers += len(entries)
                    
                    logger.info(f"âœ… ç¬¬{batch_count}æ‰¹æ¬¡è·å–äº† {len(entries)} ç¯‡è®ºæ–‡")
                    
                    if not entries:
                        logger.info("ğŸ“­ æ²¡æœ‰æ›´å¤šè®ºæ–‡å¯ç”¨")
                        break
                    
                    # Filter papers by date and parse them
                    batch_papers = []
                    older_papers = 0
                    for entry in entries:
                        paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                        
                        if paper_date < start_date:
                            older_papers += 1
                            continue
                        
                        if start_date <= paper_date <= end_date:
                            paper_data = {
                                "title": entry.title.replace('\n', ' ').strip(),
                                "abstract": entry.summary.replace('\n', ' ').strip(),
                                "authors": [author.name for author in entry.authors] if hasattr(entry, 'authors') else [],
                                "published": entry.published,
                                "updated": entry.updated,
                                "link": entry.link,
                                "arxiv_id": entry.id.split('/')[-1],
                                "categories": [tag.term for tag in entry.tags] if hasattr(entry, 'tags') else []
                            }
                            batch_papers.append(paper_data)
                    
                    all_papers.extend(batch_papers)
                    logger.info(f"ğŸ“Š ç¬¬{batch_count}æ‰¹æ¬¡ç­›é€‰ç»“æœ: {len(batch_papers)}ç¯‡åœ¨æ—¥æœŸèŒƒå›´å†…, {older_papers}ç¯‡è¿‡æ—§")
                    logger.info(f"ğŸ“ˆ ç´¯è®¡è·å–è®ºæ–‡: {len(all_papers)}ç¯‡")
                    
                    if older_papers > 0:
                        logger.info(f"ğŸ”š å‘ç°{older_papers}ç¯‡è¶…å‡ºæ—¥æœŸèŒƒå›´çš„è®ºæ–‡ï¼Œåœæ­¢æŠ“å–")
                        break
                    
                    if len(entries) < MAX_RESULTS_PER_BATCH:
                        logger.info("ğŸ”š å·²è¾¾åˆ°arXivæ•°æ®æœ«å°¾")
                        break
                    
                    start_index += MAX_RESULTS_PER_BATCH
                    
                except Exception as e:
                    logger.error(f"âŒ æŠ“å–è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    break
            
            # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
            logger.info(f"ğŸ“Š æŠ“å–æ€»ç»“:")
            logger.info(f"   - æ€»å…±å¤„ç†äº† {batch_count} ä¸ªæ‰¹æ¬¡")
            logger.info(f"   - ä»arXivè·å–äº† {total_raw_papers} ç¯‡åŸå§‹è®ºæ–‡")
            logger.info(f"   - ç­›é€‰å‡º {len(all_papers)} ç¯‡ç¬¦åˆæ—¥æœŸèŒƒå›´çš„è®ºæ–‡")
            
            # æ˜¾ç¤ºæ—¥æœŸåˆ†å¸ƒ
            if all_papers:
                dates = []
                for paper in all_papers:
                    paper_date = datetime.strptime(paper['updated'][:10], '%Y-%m-%d')
                    dates.append(paper_date.strftime('%Y-%m-%d'))
                
                date_counts = Counter(dates)
                logger.info(f"ğŸ“… è®ºæ–‡æ—¥æœŸåˆ†å¸ƒ (å‰5å¤©):")
                for date, count in date_counts.most_common(5):
                    days_ago = (datetime.now(timezone.utc).date() - datetime.strptime(date, '%Y-%m-%d').date()).days
                    logger.info(f"   - {date}: {count}ç¯‡ ({days_ago}å¤©å‰)")
            
            return all_papers
    
    # æµ‹è¯•ä¸åŒçš„æ—¶é—´èŒƒå›´
    fetcher = MockArxivFetcher()
    
    print("\nğŸ• æµ‹è¯•1: è¿‡å»1å¤©")
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=1)
    papers_1day = fetcher.fetch_papers_by_date_range(start_date, end_date, max_papers=50)
    
    print(f"\nğŸ• æµ‹è¯•2: è¿‡å»7å¤©")
    start_date = end_date - timedelta(days=7)
    papers_7days = fetcher.fetch_papers_by_date_range(start_date, end_date, max_papers=200)
    
    print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"   - è¿‡å»1å¤©: {len(papers_1day)} ç¯‡è®ºæ–‡")
    print(f"   - è¿‡å»7å¤©: {len(papers_7days)} ç¯‡è®ºæ–‡")
    print(f"   - è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ—¥å¸¸æ¨¡å¼å¾ˆå¿«å®Œæˆ!")


if __name__ == "__main__":
    test_paper_fetching_with_detailed_logs() 