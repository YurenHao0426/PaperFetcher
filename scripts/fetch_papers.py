#!/usr/bin/env python3
"""
Arxiv Paper Fetcher for LLM Bias Research
==========================================

This script fetches computer science papers from arxiv.org, filters them using 
GPT-4o to identify papers related to LLM bias and fairness, and updates a 
target GitHub repository's README with the results.

Features:
- Fetches papers from the last 24 hours (or specified days)
- Can also fetch historical papers from the past 2 years
- Uses GPT-4o for intelligent filtering
- Updates target repository via GitHub API
- Supports GitHub Actions automation
"""

import os
import sys
import json
import logging
import requests
import feedparser
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple
from github import Github
from openai import OpenAI, AsyncOpenAI
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
    ]
)
logger = logging.getLogger(__name__)

# Configuration
ARXIV_BASE_URL = "http://export.arxiv.org/api/query"
MAX_RESULTS_PER_BATCH = 100
MAX_RETRIES = 3

# Computer Science categories related to AI/ML
CS_CATEGORIES = [
    "cs.AI",  # Artificial Intelligence
    "cs.CL",  # Computation and Language
    "cs.CV",  # Computer Vision and Pattern Recognition
    "cs.LG",  # Machine Learning
    "cs.NE",  # Neural and Evolutionary Computing
    "cs.RO",  # Robotics
    "cs.IR",  # Information Retrieval
    "cs.HC",  # Human-Computer Interaction
    "stat.ML" # Machine Learning (Statistics)
]

GPT_SYSTEM_PROMPT = """You are an expert researcher in AI/ML bias, fairness, and social good applications.

Your task is to analyze a paper's title and abstract to determine if it's relevant to bias and fairness research with social good implications.

A paper is relevant if it discusses:
- Bias, fairness, or discrimination in AI/ML systems with societal impact
- Algorithmic fairness in healthcare, education, criminal justice, hiring, or finance
- Demographic bias affecting marginalized or underrepresented groups
- Data bias and its social consequences
- Ethical AI and responsible AI deployment in society
- AI safety and alignment with human values and social welfare
- Bias evaluation, auditing, or mitigation in real-world applications
- Representation and inclusion in AI systems and datasets
- Social implications of AI bias (e.g., perpetuating inequality)
- Fairness in recommendation systems, search engines, or content moderation
- Bias in computer vision, NLP, or other AI domains affecting people

The focus is on research that addresses how AI bias impacts society, vulnerable populations, or social justice, rather than purely technical ML advances without clear social relevance.

Respond with exactly "1" if the paper is relevant, or "0" if it's not relevant.
Do not include any other text in your response."""


class ArxivPaperFetcher:
    """Main class for fetching and filtering arxiv papers."""
    
    def __init__(self, openai_api_key: str):
        """Initialize the fetcher with OpenAI API key."""
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.async_openai_client = AsyncOpenAI(api_key=openai_api_key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'PaperFetcher/1.0 (https://github.com/YurenHao0426/PaperFetcher)'
        })
    
    def fetch_papers_by_date_range(self, start_date: datetime, end_date: datetime, 
                                 max_papers: int = 1000) -> List[Dict]:
        """
        Fetch papers from arxiv within a specific date range.
        
        Args:
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers: Maximum number of papers to fetch
            
        Returns:
            List of paper dictionaries
        """
        logger.info(f"ğŸ” å¼€å§‹ä»arXivæŠ“å–è®ºæ–‡: {start_date.date()} åˆ° {end_date.date()}")
        logger.info(f"ğŸ“‹ ç›®æ ‡ç±»åˆ«: {', '.join(CS_CATEGORIES)}")
        logger.info(f"ğŸ”§ æ”¹è¿›ç­–ç•¥: åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«ä»¥é¿å…ORæŸ¥è¯¢é™åˆ¶")
        
        all_papers_dict = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼Œkeyä¸ºarxiv_id
        total_categories_processed = 0
        total_raw_papers = 0
        
        # åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«
        for category in CS_CATEGORIES:
            total_categories_processed += 1
            logger.info(f"ğŸ“‚ å¤„ç†ç±»åˆ« {total_categories_processed}/{len(CS_CATEGORIES)}: {category}")
            
            category_papers = self._fetch_papers_for_category(
                category, start_date, end_date, max_papers_per_category=500
            )
            
            # åˆå¹¶åˆ°æ€»ç»“æœä¸­ï¼ˆå»é‡ï¼‰
            new_papers_count = 0
            for paper in category_papers:
                arxiv_id = paper['arxiv_id']
                if arxiv_id not in all_papers_dict:
                    all_papers_dict[arxiv_id] = paper
                    new_papers_count += 1
            
            total_raw_papers += len(category_papers)
            logger.info(f"   âœ… {category}: è·å¾—{len(category_papers)}ç¯‡, æ–°å¢{new_papers_count}ç¯‡")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¥æœŸæ’åº
        all_papers = list(all_papers_dict.values())
        all_papers.sort(key=lambda x: x['updated'], reverse=True)
        
        logger.info(f"ğŸ“Š æŠ“å–æ€»ç»“:")
        logger.info(f"   - å¤„ç†äº† {total_categories_processed} ä¸ªç±»åˆ«")
        logger.info(f"   - ä»arXivè·å–äº† {total_raw_papers} ç¯‡åŸå§‹è®ºæ–‡")
        logger.info(f"   - å»é‡åå¾—åˆ° {len(all_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        if all_papers:
            from collections import Counter
            
            # æ—¥æœŸåˆ†å¸ƒ
            dates = []
            for paper in all_papers:
                paper_date = datetime.strptime(paper['updated'][:10], '%Y-%m-%d')
                dates.append(paper_date.strftime('%Y-%m-%d'))
            
            date_counts = Counter(dates)
            logger.info(f"ğŸ“… è®ºæ–‡æ—¥æœŸåˆ†å¸ƒ (å‰5å¤©):")
            for date, count in date_counts.most_common(5):
                days_ago = (datetime.now(timezone.utc).date() - datetime.strptime(date, '%Y-%m-%d').date()).days
                logger.info(f"   - {date}: {count}ç¯‡ ({days_ago}å¤©å‰)")
            
            # ç±»åˆ«åˆ†å¸ƒ
            category_counts = Counter()
            for paper in all_papers:
                for cat in paper['categories']:
                    if cat in CS_CATEGORIES:
                        category_counts[cat] += 1
            
            logger.info(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
            for cat, count in category_counts.most_common():
                logger.info(f"   - {cat}: {count}ç¯‡")
        
        return all_papers
    
    def _fetch_papers_for_category(self, category: str, start_date: datetime, 
                                 end_date: datetime, max_papers_per_category: int = 500) -> List[Dict]:
        """
        Fetch papers for a specific category.
        
        Args:
            category: arXiv category (e.g., 'cs.AI')
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers_per_category: Maximum papers to fetch for this category
            
        Returns:
            List of paper dictionaries for this category
        """
        papers = []
        start_index = 0
        batch_count = 0
        
        while len(papers) < max_papers_per_category:
            try:
                batch_count += 1
                
                params = {
                    "search_query": f"cat:{category}",
                    "sortBy": "submittedDate",
                    "sortOrder": "descending",
                    "start": start_index,
                    "max_results": min(MAX_RESULTS_PER_BATCH, max_papers_per_category - len(papers))
                }
                
                logger.debug(f"   ğŸ“¦ {category}ç¬¬{batch_count}æ‰¹æ¬¡: ä»ç´¢å¼•{start_index}å¼€å§‹...")
                
                response = self.session.get(ARXIV_BASE_URL, params=params, timeout=30)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                entries = feed.entries
                
                logger.debug(f"   âœ… {category}ç¬¬{batch_count}æ‰¹æ¬¡è·å–äº† {len(entries)} ç¯‡è®ºæ–‡")
                
                if not entries:
                    logger.debug(f"   ğŸ“­ {category}: æ²¡æœ‰æ›´å¤šè®ºæ–‡")
                    break
                
                # Filter papers by date
                batch_papers = []
                older_papers = 0
                for entry in entries:
                    paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                    
                    if paper_date < start_date:
                        older_papers += 1
                        continue
                    
                    if start_date <= paper_date <= end_date:
                        paper_data = self._parse_paper_entry(entry)
                        batch_papers.append(paper_data)
                
                papers.extend(batch_papers)
                logger.debug(f"   ğŸ“Š {category}ç¬¬{batch_count}æ‰¹æ¬¡: {len(batch_papers)}ç¯‡ç¬¦åˆæ—¥æœŸ, {older_papers}ç¯‡è¿‡æ—§")
                
                # If we found older papers, we can stop
                if older_papers > 0:
                    logger.debug(f"   ğŸ”š {category}: å‘ç°è¿‡æ—§è®ºæ–‡ï¼Œåœæ­¢")
                    break
                
                # If we got fewer papers than requested, we've reached the end
                if len(entries) < MAX_RESULTS_PER_BATCH:
                    logger.debug(f"   ğŸ”š {category}: åˆ°è¾¾æ•°æ®æœ«å°¾")
                    break
                
                start_index += MAX_RESULTS_PER_BATCH
                
                # Safety limit per category
                if start_index >= 1000:
                    logger.debug(f"   âš ï¸ {category}: è¾¾åˆ°å•ç±»åˆ«å®‰å…¨ä¸Šé™")
                    break
                
            except Exception as e:
                logger.error(f"   âŒ {category}æŠ“å–å‡ºé”™: {e}")
                break
        
        return papers
    
    def _parse_paper_entry(self, entry) -> Dict:
        """Parse a feedparser entry into a paper dictionary."""
        return {
            "title": entry.title.replace('\n', ' ').strip(),
            "abstract": entry.summary.replace('\n', ' ').strip(),
            "authors": [author.name for author in entry.authors] if hasattr(entry, 'authors') else [],
            "published": entry.published,
            "updated": entry.updated,
            "link": entry.link,
            "arxiv_id": entry.id.split('/')[-1],
            "categories": [tag.term for tag in entry.tags] if hasattr(entry, 'tags') else []
        }
    
    def filter_papers_with_gpt(self, papers: List[Dict], use_parallel: bool = True, 
                              max_concurrent: int = 16) -> List[Dict]:
        """
        Filter papers using GPT-4o to identify bias-related research.
        
        Args:
            papers: List of paper dictionaries
            use_parallel: Whether to use parallel processing (default: True)
            max_concurrent: Maximum concurrent requests (default: 16)
            
        Returns:
            List of relevant papers
        """
        if not papers:
            logger.warning("âš ï¸ æ²¡æœ‰è®ºæ–‡éœ€è¦è¿‡æ»¤ï¼")
            return []
            
        if use_parallel and len(papers) > 5:
            logger.info(f"ğŸš€ ä½¿ç”¨å¹¶è¡Œæ¨¡å¼å¤„ç† {len(papers)} ç¯‡è®ºæ–‡ (æœ€å¤§å¹¶å‘: {max_concurrent})")
            return self._filter_papers_parallel(papers, max_concurrent)
        else:
            logger.info(f"ğŸ”„ ä½¿ç”¨ä¸²è¡Œæ¨¡å¼å¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
            return self._filter_papers_sequential(papers)
    
    def _filter_papers_sequential(self, papers: List[Dict]) -> List[Dict]:
        """Serial processing of papers (original method)."""
        logger.info(f"ğŸ¤– å¼€å§‹ä½¿ç”¨GPT-4oè¿‡æ»¤è®ºæ–‡...")
        logger.info(f"ğŸ“ å¾…å¤„ç†è®ºæ–‡æ•°é‡: {len(papers)} ç¯‡")
        
        relevant_papers = []
        processed_count = 0
        
        for i, paper in enumerate(papers, 1):
            try:
                logger.info(f"ğŸ” å¤„ç†ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡: {paper['title'][:60]}...")
                is_relevant = self._check_paper_relevance(paper)
                processed_count += 1
                
                if is_relevant:
                    relevant_papers.append(paper)
                    logger.info(f"âœ… ç¬¬ {i} ç¯‡è®ºæ–‡ [ç›¸å…³]: {paper['title'][:80]}...")
                else:
                    logger.info(f"âŒ ç¬¬ {i} ç¯‡è®ºæ–‡ [ä¸ç›¸å…³]: {paper['title'][:80]}...")
                    
                # æ¯å¤„ç†10ç¯‡è®ºæ–‡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    logger.info(f"ğŸ“Š è¿›åº¦æ›´æ–°: å·²å¤„ç† {i}/{len(papers)} ç¯‡è®ºæ–‡ï¼Œå‘ç° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†ç¬¬ {i} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ¯ GPT-4oè¿‡æ»¤å®Œæˆ!")
        logger.info(f"   - æ€»å…±å¤„ç†: {processed_count} ç¯‡è®ºæ–‡")
        logger.info(f"   - å‘ç°ç›¸å…³: {len(relevant_papers)} ç¯‡è®ºæ–‡")
        logger.info(f"   - ç›¸å…³æ¯”ä¾‹: {len(relevant_papers)/processed_count*100:.1f}%" if processed_count > 0 else "   - ç›¸å…³æ¯”ä¾‹: 0%")
        
        return relevant_papers
    
    def _filter_papers_parallel(self, papers: List[Dict], max_concurrent: int = 16) -> List[Dict]:
        """Parallel processing of papers using asyncio."""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # åœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
                import nest_asyncio
                nest_asyncio.apply()
                return loop.run_until_complete(self._async_filter_papers(papers, max_concurrent))
            else:
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                return asyncio.run(self._async_filter_papers(papers, max_concurrent))
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†æ¨¡å¼...")
            return self._filter_papers_sequential(papers)
    
    async def _async_filter_papers(self, papers: List[Dict], max_concurrent: int) -> List[Dict]:
        """Async implementation of paper filtering."""
        logger.info(f"ğŸ¤– å¼€å§‹å¼‚æ­¥GPT-4oè¿‡æ»¤...")
        logger.info(f"ğŸ“ å¾…å¤„ç†è®ºæ–‡æ•°é‡: {len(papers)} ç¯‡")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = []
        for i, paper in enumerate(papers):
            task = self._check_paper_relevance_async(paper, semaphore, i + 1, len(papers))
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # å¤„ç†ç»“æœ
        relevant_papers = []
        successful_count = 0
        error_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ç¬¬ {i+1} ç¯‡è®ºæ–‡å¤„ç†å‡ºé”™: {result}")
                error_count += 1
            elif isinstance(result, tuple):
                is_relevant, paper = result
                successful_count += 1
                if is_relevant:
                    relevant_papers.append(paper)
                    logger.debug(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡ [ç›¸å…³]: {paper['title'][:60]}...")
                else:
                    logger.debug(f"âŒ ç¬¬ {i+1} ç¯‡è®ºæ–‡ [ä¸ç›¸å…³]: {paper['title'][:60]}...")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ¯ å¹¶è¡ŒGPT-4oè¿‡æ»¤å®Œæˆ!")
        logger.info(f"   - æ€»å¤„ç†æ—¶é—´: {total_time:.1f} ç§’")
        logger.info(f"   - å¹³å‡æ¯ç¯‡: {total_time/len(papers):.2f} ç§’")
        logger.info(f"   - æˆåŠŸå¤„ç†: {successful_count} ç¯‡è®ºæ–‡")
        logger.info(f"   - å¤„ç†é”™è¯¯: {error_count} ç¯‡è®ºæ–‡")
        logger.info(f"   - å‘ç°ç›¸å…³: {len(relevant_papers)} ç¯‡è®ºæ–‡")
        
        if successful_count > 0:
            logger.info(f"   - ç›¸å…³æ¯”ä¾‹: {len(relevant_papers)/successful_count*100:.1f}%")
            
        # ä¼°ç®—åŠ é€Ÿæ•ˆæœ
        estimated_serial_time = len(papers) * 2.0  # ä¼°è®¡ä¸²è¡Œå¤„ç†æ¯ç¯‡éœ€è¦2ç§’
        speedup = estimated_serial_time / total_time if total_time > 0 else 1
        logger.info(f"   - é¢„ä¼°åŠ é€Ÿ: {speedup:.1f}x")
        
        return relevant_papers
    
    async def _check_paper_relevance_async(self, paper: Dict, semaphore: asyncio.Semaphore, 
                                         index: int, total: int) -> tuple:
        """Async version of paper relevance checking."""
        async with semaphore:
            try:
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ç¯‡æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                if index % 10 == 0:
                    logger.info(f"ğŸ“Š å¹¶è¡Œè¿›åº¦: {index}/{total} ç¯‡è®ºæ–‡å¤„ç†ä¸­...")
                
                prompt = f"Title: {paper['title']}\n\nAbstract: {paper['abstract']}"
                
                response = await self.async_openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": GPT_SYSTEM_PROMPT},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_tokens=1
                )
                
                result = response.choices[0].message.content.strip()
                is_relevant = result == "1"
                
                logger.debug(f"GPT-4oå“åº” #{index}: '{result}' -> {'ç›¸å…³' if is_relevant else 'ä¸ç›¸å…³'}")
                return (is_relevant, paper)
                
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {index} ç¯‡è®ºæ–‡å¼‚æ­¥å¤„ç†å‡ºé”™: {e}")
                # è¿”å›å¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
                raise e
    
    def _check_paper_relevance(self, paper: Dict) -> bool:
        """Check if a paper is relevant using GPT-4o."""
        prompt = f"Title: {paper['title']}\n\nAbstract: {paper['abstract']}"
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": GPT_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=1
            )
            
            result = response.choices[0].message.content.strip()
            is_relevant = result == "1"
            
            logger.debug(f"GPT-4oå“åº”: '{result}' -> {'ç›¸å…³' if is_relevant else 'ä¸ç›¸å…³'}")
            return is_relevant
            
        except Exception as e:
            logger.error(f"è°ƒç”¨GPT-4o APIæ—¶å‡ºé”™: {e}")
            return False
    
    def fetch_recent_papers(self, days: int = 1) -> List[Dict]:
        """Fetch papers from the last N days."""
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=days)
        
        logger.info(f"ğŸ“… æ—¥å¸¸æ¨¡å¼: è·å– {days} å¤©å†…çš„è®ºæ–‡")
        logger.info(f"ğŸ• æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d %H:%M')} UTC ~ {end_date.strftime('%Y-%m-%d %H:%M')} UTC")
        
        papers = self.fetch_papers_by_date_range(start_date, end_date)
        
        if papers:
            logger.info(f"ğŸ“‹ å¼€å§‹GPT-4oæ™ºèƒ½è¿‡æ»¤é˜¶æ®µ...")
            
            # ä»ç¯å¢ƒå˜é‡è·å–å¹¶è¡Œè®¾ç½®
            use_parallel = os.getenv("USE_PARALLEL", "true").lower() == "true"
            max_concurrent = int(os.getenv("MAX_CONCURRENT", "16"))
            
            return self.filter_papers_with_gpt(papers, use_parallel=use_parallel, 
                                             max_concurrent=max_concurrent)
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè·³è¿‡GPTè¿‡æ»¤æ­¥éª¤")
            return []
    
    def fetch_historical_papers(self, years: int = 2) -> List[Dict]:
        """Fetch papers from the past N years."""
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=years * 365)
        
        # ä»ç¯å¢ƒå˜é‡è·å–é™åˆ¶é…ç½®
        max_papers = int(os.getenv("MAX_HISTORICAL_PAPERS", "50000"))  # é»˜è®¤50000ç¯‡
        max_per_category = int(os.getenv("MAX_PAPERS_PER_CATEGORY", "10000"))  # æ¯ç±»åˆ«10000ç¯‡
        
        logger.info(f"ğŸ“š å†å²æ¨¡å¼: è·å–è¿‡å» {years} å¹´çš„è®ºæ–‡")
        logger.info(f"ğŸ• æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"ğŸ“Š é…ç½®é™åˆ¶:")
        logger.info(f"   - æœ€å¤§è®ºæ–‡æ•°: {max_papers:,} ç¯‡")
        logger.info(f"   - æ¯ç±»åˆ«é™åˆ¶: {max_per_category:,} ç¯‡")
        
        if max_papers >= 20000:
            logger.info(f"âš ï¸ å¤§è§„æ¨¡å†å²æ¨¡å¼: è¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´å’Œå¤§é‡APIè°ƒç”¨")
            logger.info(f"ğŸ’¡ å»ºè®®: å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´é™åˆ¶")
            logger.info(f"   - MAX_HISTORICAL_PAPERS={max_papers}")
            logger.info(f"   - MAX_PAPERS_PER_CATEGORY={max_per_category}")
        
        papers = self.fetch_papers_by_date_range_unlimited(
            start_date, end_date, max_papers=max_papers, max_per_category=max_per_category
        )
        
        if papers:
            logger.info(f"ğŸ“‹ å¼€å§‹GPT-4oæ™ºèƒ½è¿‡æ»¤é˜¶æ®µ...")
            
            # å†å²æ¨¡å¼é»˜è®¤ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°ï¼ˆå› ä¸ºè®ºæ–‡æ•°é‡å¤šï¼‰
            use_parallel = os.getenv("USE_PARALLEL", "true").lower() == "true"
            max_concurrent = int(os.getenv("MAX_CONCURRENT", "50"))  # å†å²æ¨¡å¼é»˜è®¤æ›´é«˜å¹¶å‘
            
            return self.filter_papers_with_gpt(papers, use_parallel=use_parallel, 
                                             max_concurrent=max_concurrent)
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè·³è¿‡GPTè¿‡æ»¤æ­¥éª¤")
            return []

    def fetch_papers_by_date_range_unlimited(self, start_date: datetime, end_date: datetime, 
                                           max_papers: int = 50000, max_per_category: int = 10000) -> List[Dict]:
        """
        Fetch papers by date range with higher limits for historical mode.
        
        Args:
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers: Maximum total papers to fetch
            max_per_category: Maximum papers per category
            
        Returns:
            List of paper dictionaries
        """
        logger.info(f"ğŸ” å¼€å§‹è·å–è®ºæ–‡ - æ— é™åˆ¶æ¨¡å¼")
        logger.info(f"ğŸ• æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d %H:%M')} UTC ~ {end_date.strftime('%Y-%m-%d %H:%M')} UTC")
        logger.info(f"ğŸ“Š æœç´¢é…ç½®:")
        logger.info(f"   - æœ€å¤§è®ºæ–‡æ•°: {max_papers:,}")
        logger.info(f"   - æ¯ç±»åˆ«é™åˆ¶: {max_per_category:,}")
        logger.info(f"   - æœç´¢ç±»åˆ«: {len(CS_CATEGORIES)} ä¸ª")
        
        all_papers_dict = {}  # ä½¿ç”¨å­—å…¸å»é‡
        total_raw_papers = 0
        total_categories_processed = 0
        
        # åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«
        for category in CS_CATEGORIES:
            total_categories_processed += 1
            logger.info(f"ğŸ“‚ å¤„ç†ç±»åˆ« {total_categories_processed}/{len(CS_CATEGORIES)}: {category}")
            
            category_papers = self._fetch_papers_for_category_unlimited(
                category, start_date, end_date, max_papers_per_category=max_per_category
            )
            
            # åˆå¹¶åˆ°æ€»ç»“æœä¸­ï¼ˆå»é‡ï¼‰
            new_papers_count = 0
            for paper in category_papers:
                arxiv_id = paper['arxiv_id']
                if arxiv_id not in all_papers_dict:
                    all_papers_dict[arxiv_id] = paper
                    new_papers_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€»æ•°é™åˆ¶
                    if len(all_papers_dict) >= max_papers:
                        logger.info(f"âš ï¸ è¾¾åˆ°æœ€å¤§è®ºæ–‡æ•° {max_papers:,}ï¼Œåœæ­¢è·å–")
                        break
            
            total_raw_papers += len(category_papers)
            logger.info(f"   âœ… {category}: è·å¾—{len(category_papers):,}ç¯‡, æ–°å¢{new_papers_count:,}ç¯‡")
            
            # å¦‚æœè¾¾åˆ°æ€»æ•°é™åˆ¶ï¼Œåœæ­¢
            if len(all_papers_dict) >= max_papers:
                break
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¥æœŸæ’åº
        all_papers = list(all_papers_dict.values())
        all_papers.sort(key=lambda x: x['updated'], reverse=True)
        
        logger.info(f"ğŸ“Š æŠ“å–æ€»ç»“:")
        logger.info(f"   - å¤„ç†äº† {total_categories_processed} ä¸ªç±»åˆ«")
        logger.info(f"   - ä»arXivè·å–äº† {total_raw_papers:,} ç¯‡åŸå§‹è®ºæ–‡")
        logger.info(f"   - å»é‡åå¾—åˆ° {len(all_papers):,} ç¯‡å”¯ä¸€è®ºæ–‡")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        if all_papers:
            from collections import Counter
            
            # æ—¥æœŸåˆ†å¸ƒ
            dates = []
            for paper in all_papers:
                paper_date = datetime.strptime(paper['updated'][:10], '%Y-%m-%d')
                dates.append(paper_date.strftime('%Y-%m-%d'))
            
            date_counts = Counter(dates)
            logger.info(f"ğŸ“… è®ºæ–‡æ—¥æœŸåˆ†å¸ƒ (å‰10å¤©):")
            for date, count in date_counts.most_common(10):
                days_ago = (datetime.now(timezone.utc).date() - datetime.strptime(date, '%Y-%m-%d').date()).days
                logger.info(f"   - {date}: {count:,}ç¯‡ ({days_ago}å¤©å‰)")
            
            # ç±»åˆ«åˆ†å¸ƒ
            category_counts = Counter()
            for paper in all_papers:
                for cat in paper['categories']:
                    if cat in CS_CATEGORIES:
                        category_counts[cat] += 1
            
            logger.info(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
            for cat, count in category_counts.most_common():
                logger.info(f"   - {cat}: {count:,}ç¯‡")
        
        return all_papers

    def _fetch_papers_for_category_unlimited(self, category: str, start_date: datetime, 
                                           end_date: datetime, max_papers_per_category: int = 10000) -> List[Dict]:
        """
        Fetch papers for a specific category with higher limits.
        
        Args:
            category: arXiv category (e.g., 'cs.AI')
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers_per_category: Maximum papers to fetch for this category
            
        Returns:
            List of paper dictionaries for this category
        """
        papers = []
        start_index = 0
        batch_count = 0
        api_calls = 0
        max_api_calls = max_papers_per_category // MAX_RESULTS_PER_BATCH + 100  # åŠ¨æ€è®¡ç®—APIè°ƒç”¨é™åˆ¶
        
        logger.info(f"   ğŸ¯ {category}: å¼€å§‹è·å–ï¼Œç›®æ ‡æœ€å¤š{max_papers_per_category:,}ç¯‡è®ºæ–‡")
        
        while len(papers) < max_papers_per_category and api_calls < max_api_calls:
            try:
                batch_count += 1
                api_calls += 1
                
                params = {
                    "search_query": f"cat:{category}",
                    "sortBy": "submittedDate",
                    "sortOrder": "descending",
                    "start": start_index,
                    "max_results": min(MAX_RESULTS_PER_BATCH, max_papers_per_category - len(papers))
                }
                
                if batch_count % 10 == 0:  # æ¯10æ‰¹æ¬¡æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†è¿›åº¦
                    logger.info(f"   ğŸ“¦ {category}ç¬¬{batch_count}æ‰¹æ¬¡: ä»ç´¢å¼•{start_index}å¼€å§‹ï¼Œå·²è·å–{len(papers):,}ç¯‡...")
                
                response = self.session.get(ARXIV_BASE_URL, params=params, timeout=30)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                entries = feed.entries
                
                logger.debug(f"   âœ… {category}ç¬¬{batch_count}æ‰¹æ¬¡è·å–äº† {len(entries)} ç¯‡è®ºæ–‡")
                
                if not entries:
                    logger.debug(f"   ğŸ“­ {category}: æ²¡æœ‰æ›´å¤šè®ºæ–‡")
                    break
                
                # Filter papers by date
                batch_papers = []
                older_papers = 0
                for entry in entries:
                    paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                    
                    if paper_date < start_date:
                        older_papers += 1
                        continue
                    
                    if start_date <= paper_date <= end_date:
                        paper_data = self._parse_paper_entry(entry)
                        batch_papers.append(paper_data)
                
                papers.extend(batch_papers)
                logger.debug(f"   ğŸ“Š {category}ç¬¬{batch_count}æ‰¹æ¬¡: {len(batch_papers)}ç¯‡ç¬¦åˆæ—¥æœŸ, {older_papers}ç¯‡è¿‡æ—§")
                
                # If we found older papers, we can stop
                if older_papers > 0:
                    logger.debug(f"   ğŸ”š {category}: å‘ç°è¿‡æ—§è®ºæ–‡ï¼Œåœæ­¢")
                    break
                
                # If we got fewer papers than requested, we've reached the end
                if len(entries) < MAX_RESULTS_PER_BATCH:
                    logger.debug(f"   ğŸ”š {category}: åˆ°è¾¾æ•°æ®æœ«å°¾")
                    break
                
                start_index += MAX_RESULTS_PER_BATCH
                
            except Exception as e:
                logger.error(f"   âŒ {category}æŠ“å–å‡ºé”™: {e}")
                break
        
        logger.info(f"   âœ… {category}: å®Œæˆï¼Œè·å–{len(papers):,}ç¯‡è®ºæ–‡ (APIè°ƒç”¨{api_calls}æ¬¡)")
        return papers


class GitHubUpdater:
    """Handle GitHub repository updates."""
    
    def __init__(self, token: str, repo_name: str):
        """Initialize GitHub updater."""
        self.github = Github(token)
        self.repo_name = repo_name
        self.repo = self.github.get_repo(repo_name)
    
    def update_readme_with_papers(self, papers: List[Dict], section_title: str = None):
        """Update README with new papers in reverse chronological order (newest first)."""
        if not papers:
            logger.info("No papers to add to README")
            return
        
        if section_title is None:
            section_title = f"Papers Updated on {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}"
        
        try:
            # Get current README
            readme_file = self.repo.get_contents("README.md", ref="main")
            current_content = readme_file.decoded_content.decode("utf-8")
            
            # Create new section
            new_section = f"\n\n## {section_title}\n\n"
            
            for paper in papers:
                # Format paper entry
                authors_str = ", ".join(paper['authors'][:3])  # First 3 authors
                if len(paper['authors']) > 3:
                    authors_str += " et al."
                
                categories_str = ", ".join(paper['categories'])
                
                new_section += f"### {paper['title']}\n\n"
                new_section += f"**Authors:** {authors_str}\n\n"
                new_section += f"**Categories:** {categories_str}\n\n"
                new_section += f"**Published:** {paper['published']}\n\n"
                new_section += f"**Abstract:** {paper['abstract']}\n\n"
                new_section += f"**Link:** [arXiv:{paper['arxiv_id']}]({paper['link']})\n\n"
                new_section += "---\n\n"
            
            # Insert new papers at the beginning to maintain reverse chronological order
            # Find the end of the main documentation (after the project description and setup)
            insert_position = self._find_papers_insert_position(current_content)
            
            if insert_position > 0:
                # Insert new section after the main documentation but before existing papers
                updated_content = (current_content[:insert_position] + 
                                 new_section + 
                                 current_content[insert_position:])
                logger.info(f"ğŸ“ æ–°è®ºæ–‡æ®µè½æ’å…¥åˆ°READMEå¼€å¤´ï¼Œä¿æŒæ—¶é—´å€’åº")
            else:
                # Fallback: append to end if can't find proper insertion point
                updated_content = current_content + new_section
                logger.info(f"ğŸ“ æ–°è®ºæ–‡æ®µè½è¿½åŠ åˆ°READMEæœ«å°¾ï¼ˆæ‰¾ä¸åˆ°åˆé€‚æ’å…¥ä½ç½®ï¼‰")
            
            commit_message = f"Auto-update: Added {len(papers)} new papers on {datetime.now(timezone.utc).strftime('%Y-%m-%d')}"
            
            self.repo.update_file(
                path="README.md",
                message=commit_message,
                content=updated_content,
                sha=readme_file.sha,
                branch="main"
            )
            
            logger.info(f"âœ… æˆåŠŸæ›´æ–°READMEï¼Œæ·»åŠ äº† {len(papers)} ç¯‡è®ºæ–‡ (æ—¶é—´å€’åº)")
            
        except Exception as e:
            logger.error(f"Error updating README: {e}")
            raise
    
    def _find_papers_insert_position(self, content: str) -> int:
        """Find the best position to insert new papers (after main doc, before existing papers)."""
        lines = content.split('\n')
        
        # Look for patterns that indicate the end of documentation and start of papers
        # Search in order of priority
        insert_patterns = [
            "**Note**: This tool is designed for academic research purposes",  # End of README
            "## Papers Updated on",  # Existing paper sections
            "## Historical",  # Historical paper sections
            "### ",  # Any section that might be a paper title
            "---",  # Common separator before papers
        ]
        
        for pattern in insert_patterns:
            for i, line in enumerate(lines):
                if pattern in line:
                    # Found a good insertion point - insert before this line
                    # Convert line index to character position
                    char_position = sum(len(lines[j]) + 1 for j in range(i))  # +1 for newline
                    return char_position
        
        # If no patterns found, try to find end of main documentation
        # Look for the end of the last documentation section
        last_doc_section = -1
        for i, line in enumerate(lines):
            if line.startswith('## ') and not line.startswith('## Papers') and not line.startswith('## Historical'):
                last_doc_section = i
        
        if last_doc_section >= 0:
            # Find the end of this documentation section
            section_end = len(lines)
            for i in range(last_doc_section + 1, len(lines)):
                if lines[i].startswith('## '):
                    section_end = i
                    break
            
            # Insert after this section
            char_position = sum(len(lines[j]) + 1 for j in range(section_end))
            return char_position
        
        # Final fallback: return 0 to trigger append behavior
        return 0


def main():
    """Main function to run the paper fetcher."""
    import time
    
    start_time = time.time()
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒArXivè®ºæ–‡æŠ“å–ä»»åŠ¡")
    logger.info("=" * 60)
    
    # Get environment variables
    openai_api_key = os.getenv("OPENAI_API_KEY")
    github_token = os.getenv("TARGET_REPO_TOKEN")
    target_repo = os.getenv("TARGET_REPO_NAME", "YurenHao0426/awesome-llm-bias-papers")
    
    logger.info("ğŸ”§ é…ç½®ä¿¡æ¯:")
    logger.info(f"   - OpenAI API Key: {'å·²è®¾ç½®' if openai_api_key else 'æœªè®¾ç½®'}")
    logger.info(f"   - GitHub Token: {'å·²è®¾ç½®' if github_token else 'æœªè®¾ç½®'}")
    logger.info(f"   - ç›®æ ‡ä»“åº“: {target_repo}")
    
    # Check for required environment variables
    if not openai_api_key:
        logger.error("âŒ OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        sys.exit(1)
    
    if not github_token:
        logger.error("âŒ TARGET_REPO_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        sys.exit(1)
    
    # Get command line arguments
    mode = os.getenv("FETCH_MODE", "daily")  # daily or historical
    days = int(os.getenv("FETCH_DAYS", "1"))
    
    logger.info(f"ğŸ“‹ æ‰§è¡Œæ¨¡å¼: {mode}")
    if mode == "daily":
        logger.info(f"ğŸ“… æŠ“å–å¤©æ•°: {days} å¤©")
    
    try:
        step_start = time.time()
        
        # Initialize fetcher
        logger.info("ğŸ”„ åˆå§‹åŒ–è®ºæ–‡æŠ“å–å™¨...")
        fetcher = ArxivPaperFetcher(openai_api_key)
        logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆ ({time.time() - step_start:.1f}ç§’)")
        
        # Fetch papers
        step_start = time.time()
        if mode == "historical":
            logger.info("ğŸ“š è¿è¡Œå†å²æ¨¡å¼ - æŠ“å–è¿‡å»2å¹´çš„è®ºæ–‡")
            papers = fetcher.fetch_historical_papers(years=2)
            section_title = "Historical LLM Bias Papers (Past 2 Years)"
        else:
            logger.info(f"ğŸ“° è¿è¡Œæ—¥å¸¸æ¨¡å¼ - æŠ“å–è¿‡å»{days}å¤©çš„è®ºæ–‡")
            papers = fetcher.fetch_recent_papers(days=days)
            section_title = None  # Use default timestamp
        
        fetch_time = time.time() - step_start
        logger.info(f"â±ï¸ è®ºæ–‡æŠ“å–å’Œè¿‡æ»¤å®Œæˆ ({fetch_time:.1f}ç§’)")
        
        # Update GitHub repository
        if papers:
            step_start = time.time()
            logger.info(f"ğŸ“¤ å¼€å§‹æ›´æ–°GitHubä»“åº“...")
            updater = GitHubUpdater(github_token, target_repo)
            updater.update_readme_with_papers(papers, section_title)
            update_time = time.time() - step_start
            logger.info(f"âœ… GitHubä»“åº“æ›´æ–°å®Œæˆ ({update_time:.1f}ç§’)")
            
            logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆ!")
            logger.info(f"   - æ‰¾åˆ°ç›¸å…³è®ºæ–‡: {len(papers)} ç¯‡")
            logger.info(f"   - æ€»æ‰§è¡Œæ—¶é—´: {time.time() - start_time:.1f} ç§’")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            logger.info("å¯èƒ½çš„åŸå› :")
            logger.info("   - æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ–°çš„LLMåè§ç›¸å…³è®ºæ–‡")
            logger.info("   - arXiv APIè¿æ¥é—®é¢˜")
            logger.info("   - GPT-4oè¿‡æ»¤æ¡ä»¶è¿‡äºä¸¥æ ¼")
            logger.info(f"   - æ€»æ‰§è¡Œæ—¶é—´: {time.time() - start_time:.1f} ç§’")
            
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
