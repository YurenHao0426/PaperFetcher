#!/usr/bin/env python3
"""
Arxiv Paper Fetcher for LLM Bias Research
==========================================

This script fetches computer science papers from arxiv.org, filters them using 
GPT-4o to identify papers related to LLM bias and fairness, and updates a 
target GitHub repository's README with the results.

Features:
- Fetches papers from the last 24 hours (or specified days)
- Can also fetch historical papers from the past 2 years
- Uses GPT-4o for intelligent filtering
- Updates target repository via GitHub API
- Supports GitHub Actions automation
"""

import os
import sys
import json
import logging
import requests
import feedparser
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple
from github import Github
from openai import OpenAI

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
    ]
)
logger = logging.getLogger(__name__)

# Configuration
ARXIV_BASE_URL = "http://export.arxiv.org/api/query"
MAX_RESULTS_PER_BATCH = 100
MAX_RETRIES = 3

# Computer Science categories related to AI/ML
CS_CATEGORIES = [
    "cs.AI",  # Artificial Intelligence
    "cs.CL",  # Computation and Language
    "cs.CV",  # Computer Vision and Pattern Recognition
    "cs.LG",  # Machine Learning
    "cs.NE",  # Neural and Evolutionary Computing
    "cs.RO",  # Robotics
    "cs.IR",  # Information Retrieval
    "cs.HC",  # Human-Computer Interaction
    "stat.ML" # Machine Learning (Statistics)
]

GPT_SYSTEM_PROMPT = """You are an expert researcher in AI/ML bias and fairness. 

Your task is to analyze a paper's title and abstract to determine if it's relevant to LLM (Large Language Model) bias and fairness research.

A paper is relevant if it discusses:
- Bias in large language models, generative AI, or foundation models
- Fairness issues in NLP models or text generation
- Ethical concerns with language models
- Demographic bias in AI systems
- Alignment and safety of language models
- Bias evaluation or mitigation in NLP

Respond with exactly "1" if the paper is relevant, or "0" if it's not relevant.
Do not include any other text in your response."""


class ArxivPaperFetcher:
    """Main class for fetching and filtering arxiv papers."""
    
    def __init__(self, openai_api_key: str):
        """Initialize the fetcher with OpenAI API key."""
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'PaperFetcher/1.0 (https://github.com/YurenHao0426/PaperFetcher)'
        })
    
    def fetch_papers_by_date_range(self, start_date: datetime, end_date: datetime, 
                                 max_papers: int = 1000) -> List[Dict]:
        """
        Fetch papers from arxiv within a specific date range.
        
        Args:
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers: Maximum number of papers to fetch
            
        Returns:
            List of paper dictionaries
        """
        logger.info(f"ğŸ” å¼€å§‹ä»arXivæŠ“å–è®ºæ–‡: {start_date.date()} åˆ° {end_date.date()}")
        logger.info(f"ğŸ“‹ ç›®æ ‡ç±»åˆ«: {', '.join(CS_CATEGORIES)}")
        logger.info(f"ğŸ”§ æ”¹è¿›ç­–ç•¥: åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«ä»¥é¿å…ORæŸ¥è¯¢é™åˆ¶")
        
        all_papers_dict = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼Œkeyä¸ºarxiv_id
        total_categories_processed = 0
        total_raw_papers = 0
        
        # åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«
        for category in CS_CATEGORIES:
            total_categories_processed += 1
            logger.info(f"ğŸ“‚ å¤„ç†ç±»åˆ« {total_categories_processed}/{len(CS_CATEGORIES)}: {category}")
            
            category_papers = self._fetch_papers_for_category(
                category, start_date, end_date, max_papers_per_category=500
            )
            
            # åˆå¹¶åˆ°æ€»ç»“æœä¸­ï¼ˆå»é‡ï¼‰
            new_papers_count = 0
            for paper in category_papers:
                arxiv_id = paper['arxiv_id']
                if arxiv_id not in all_papers_dict:
                    all_papers_dict[arxiv_id] = paper
                    new_papers_count += 1
            
            total_raw_papers += len(category_papers)
            logger.info(f"   âœ… {category}: è·å¾—{len(category_papers)}ç¯‡, æ–°å¢{new_papers_count}ç¯‡")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¥æœŸæ’åº
        all_papers = list(all_papers_dict.values())
        all_papers.sort(key=lambda x: x['updated'], reverse=True)
        
        logger.info(f"ğŸ“Š æŠ“å–æ€»ç»“:")
        logger.info(f"   - å¤„ç†äº† {total_categories_processed} ä¸ªç±»åˆ«")
        logger.info(f"   - ä»arXivè·å–äº† {total_raw_papers} ç¯‡åŸå§‹è®ºæ–‡")
        logger.info(f"   - å»é‡åå¾—åˆ° {len(all_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        if all_papers:
            from collections import Counter
            
            # æ—¥æœŸåˆ†å¸ƒ
            dates = []
            for paper in all_papers:
                paper_date = datetime.strptime(paper['updated'][:10], '%Y-%m-%d')
                dates.append(paper_date.strftime('%Y-%m-%d'))
            
            date_counts = Counter(dates)
            logger.info(f"ğŸ“… è®ºæ–‡æ—¥æœŸåˆ†å¸ƒ (å‰5å¤©):")
            for date, count in date_counts.most_common(5):
                days_ago = (datetime.now(timezone.utc).date() - datetime.strptime(date, '%Y-%m-%d').date()).days
                logger.info(f"   - {date}: {count}ç¯‡ ({days_ago}å¤©å‰)")
            
            # ç±»åˆ«åˆ†å¸ƒ
            category_counts = Counter()
            for paper in all_papers:
                for cat in paper['categories']:
                    if cat in CS_CATEGORIES:
                        category_counts[cat] += 1
            
            logger.info(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
            for cat, count in category_counts.most_common():
                logger.info(f"   - {cat}: {count}ç¯‡")
        
        return all_papers
    
    def _fetch_papers_for_category(self, category: str, start_date: datetime, 
                                 end_date: datetime, max_papers_per_category: int = 500) -> List[Dict]:
        """
        Fetch papers for a specific category.
        
        Args:
            category: arXiv category (e.g., 'cs.AI')
            start_date: Start date for paper search
            end_date: End date for paper search
            max_papers_per_category: Maximum papers to fetch for this category
            
        Returns:
            List of paper dictionaries for this category
        """
        papers = []
        start_index = 0
        batch_count = 0
        
        while len(papers) < max_papers_per_category:
            try:
                batch_count += 1
                
                params = {
                    "search_query": f"cat:{category}",
                    "sortBy": "submittedDate",
                    "sortOrder": "descending",
                    "start": start_index,
                    "max_results": min(MAX_RESULTS_PER_BATCH, max_papers_per_category - len(papers))
                }
                
                logger.debug(f"   ğŸ“¦ {category}ç¬¬{batch_count}æ‰¹æ¬¡: ä»ç´¢å¼•{start_index}å¼€å§‹...")
                
                response = self.session.get(ARXIV_BASE_URL, params=params, timeout=30)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                entries = feed.entries
                
                logger.debug(f"   âœ… {category}ç¬¬{batch_count}æ‰¹æ¬¡è·å–äº† {len(entries)} ç¯‡è®ºæ–‡")
                
                if not entries:
                    logger.debug(f"   ğŸ“­ {category}: æ²¡æœ‰æ›´å¤šè®ºæ–‡")
                    break
                
                # Filter papers by date
                batch_papers = []
                older_papers = 0
                for entry in entries:
                    paper_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                    
                    if paper_date < start_date:
                        older_papers += 1
                        continue
                    
                    if start_date <= paper_date <= end_date:
                        paper_data = self._parse_paper_entry(entry)
                        batch_papers.append(paper_data)
                
                papers.extend(batch_papers)
                logger.debug(f"   ğŸ“Š {category}ç¬¬{batch_count}æ‰¹æ¬¡: {len(batch_papers)}ç¯‡ç¬¦åˆæ—¥æœŸ, {older_papers}ç¯‡è¿‡æ—§")
                
                # If we found older papers, we can stop
                if older_papers > 0:
                    logger.debug(f"   ğŸ”š {category}: å‘ç°è¿‡æ—§è®ºæ–‡ï¼Œåœæ­¢")
                    break
                
                # If we got fewer papers than requested, we've reached the end
                if len(entries) < MAX_RESULTS_PER_BATCH:
                    logger.debug(f"   ğŸ”š {category}: åˆ°è¾¾æ•°æ®æœ«å°¾")
                    break
                
                start_index += MAX_RESULTS_PER_BATCH
                
                # Safety limit per category
                if start_index >= 1000:
                    logger.debug(f"   âš ï¸ {category}: è¾¾åˆ°å•ç±»åˆ«å®‰å…¨ä¸Šé™")
                    break
                
            except Exception as e:
                logger.error(f"   âŒ {category}æŠ“å–å‡ºé”™: {e}")
                break
        
        return papers
    
    def _parse_paper_entry(self, entry) -> Dict:
        """Parse a feedparser entry into a paper dictionary."""
        return {
            "title": entry.title.replace('\n', ' ').strip(),
            "abstract": entry.summary.replace('\n', ' ').strip(),
            "authors": [author.name for author in entry.authors] if hasattr(entry, 'authors') else [],
            "published": entry.published,
            "updated": entry.updated,
            "link": entry.link,
            "arxiv_id": entry.id.split('/')[-1],
            "categories": [tag.term for tag in entry.tags] if hasattr(entry, 'tags') else []
        }
    
    def filter_papers_with_gpt(self, papers: List[Dict]) -> List[Dict]:
        """
        Filter papers using GPT-4o to identify bias-related research.
        
        Args:
            papers: List of paper dictionaries
            
        Returns:
            List of relevant papers
        """
        if not papers:
            logger.warning("âš ï¸ æ²¡æœ‰è®ºæ–‡éœ€è¦è¿‡æ»¤ï¼")
            return []
            
        logger.info(f"ğŸ¤– å¼€å§‹ä½¿ç”¨GPT-4oè¿‡æ»¤è®ºæ–‡...")
        logger.info(f"ğŸ“ å¾…å¤„ç†è®ºæ–‡æ•°é‡: {len(papers)} ç¯‡")
        
        relevant_papers = []
        processed_count = 0
        
        for i, paper in enumerate(papers, 1):
            try:
                logger.info(f"ğŸ” å¤„ç†ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡: {paper['title'][:60]}...")
                is_relevant = self._check_paper_relevance(paper)
                processed_count += 1
                
                if is_relevant:
                    relevant_papers.append(paper)
                    logger.info(f"âœ… ç¬¬ {i} ç¯‡è®ºæ–‡ [ç›¸å…³]: {paper['title'][:80]}...")
                else:
                    logger.info(f"âŒ ç¬¬ {i} ç¯‡è®ºæ–‡ [ä¸ç›¸å…³]: {paper['title'][:80]}...")
                    
                # æ¯å¤„ç†10ç¯‡è®ºæ–‡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    logger.info(f"ğŸ“Š è¿›åº¦æ›´æ–°: å·²å¤„ç† {i}/{len(papers)} ç¯‡è®ºæ–‡ï¼Œå‘ç° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†ç¬¬ {i} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ¯ GPT-4oè¿‡æ»¤å®Œæˆ!")
        logger.info(f"   - æ€»å…±å¤„ç†: {processed_count} ç¯‡è®ºæ–‡")
        logger.info(f"   - å‘ç°ç›¸å…³: {len(relevant_papers)} ç¯‡è®ºæ–‡")
        logger.info(f"   - ç›¸å…³æ¯”ä¾‹: {len(relevant_papers)/processed_count*100:.1f}%" if processed_count > 0 else "   - ç›¸å…³æ¯”ä¾‹: 0%")
        
        return relevant_papers
    
    def _check_paper_relevance(self, paper: Dict) -> bool:
        """Check if a paper is relevant using GPT-4o."""
        prompt = f"Title: {paper['title']}\n\nAbstract: {paper['abstract']}"
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": GPT_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=1
            )
            
            result = response.choices[0].message.content.strip()
            is_relevant = result == "1"
            
            logger.debug(f"GPT-4oå“åº”: '{result}' -> {'ç›¸å…³' if is_relevant else 'ä¸ç›¸å…³'}")
            return is_relevant
            
        except Exception as e:
            logger.error(f"è°ƒç”¨GPT-4o APIæ—¶å‡ºé”™: {e}")
            return False
    
    def fetch_recent_papers(self, days: int = 1) -> List[Dict]:
        """Fetch papers from the last N days."""
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=days)
        
        logger.info(f"ğŸ“… æ—¥å¸¸æ¨¡å¼: è·å– {days} å¤©å†…çš„è®ºæ–‡")
        logger.info(f"ğŸ• æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d %H:%M')} UTC ~ {end_date.strftime('%Y-%m-%d %H:%M')} UTC")
        
        papers = self.fetch_papers_by_date_range(start_date, end_date)
        
        if papers:
            logger.info(f"ğŸ“‹ å¼€å§‹GPT-4oæ™ºèƒ½è¿‡æ»¤é˜¶æ®µ...")
            return self.filter_papers_with_gpt(papers)
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè·³è¿‡GPTè¿‡æ»¤æ­¥éª¤")
            return []
    
    def fetch_historical_papers(self, years: int = 2) -> List[Dict]:
        """Fetch papers from the past N years."""
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=years * 365)
        
        logger.info(f"ğŸ“š å†å²æ¨¡å¼: è·å–è¿‡å» {years} å¹´çš„è®ºæ–‡")
        logger.info(f"ğŸ• æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"âš ï¸ æ³¨æ„: å†å²æ¨¡å¼æœ€å¤šå¤„ç† 5000 ç¯‡è®ºæ–‡ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        
        papers = self.fetch_papers_by_date_range(start_date, end_date, max_papers=5000)
        
        if papers:
            logger.info(f"ğŸ“‹ å¼€å§‹GPT-4oæ™ºèƒ½è¿‡æ»¤é˜¶æ®µ...")
            return self.filter_papers_with_gpt(papers)
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè·³è¿‡GPTè¿‡æ»¤æ­¥éª¤")
            return []


class GitHubUpdater:
    """Handle GitHub repository updates."""
    
    def __init__(self, token: str, repo_name: str):
        """Initialize GitHub updater."""
        self.github = Github(token)
        self.repo_name = repo_name
        self.repo = self.github.get_repo(repo_name)
    
    def update_readme_with_papers(self, papers: List[Dict], section_title: str = None):
        """Update README with new papers."""
        if not papers:
            logger.info("No papers to add to README")
            return
        
        if section_title is None:
            section_title = f"Papers Updated on {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}"
        
        try:
            # Get current README
            readme_file = self.repo.get_contents("README.md", ref="main")
            current_content = readme_file.decoded_content.decode("utf-8")
            
            # Create new section
            new_section = f"\n\n## {section_title}\n\n"
            
            for paper in papers:
                # Format paper entry
                authors_str = ", ".join(paper['authors'][:3])  # First 3 authors
                if len(paper['authors']) > 3:
                    authors_str += " et al."
                
                categories_str = ", ".join(paper['categories'])
                
                new_section += f"### {paper['title']}\n\n"
                new_section += f"**Authors:** {authors_str}\n\n"
                new_section += f"**Categories:** {categories_str}\n\n"
                new_section += f"**Published:** {paper['published']}\n\n"
                new_section += f"**Abstract:** {paper['abstract']}\n\n"
                new_section += f"**Link:** [arXiv:{paper['arxiv_id']}]({paper['link']})\n\n"
                new_section += "---\n\n"
            
            # Update README
            updated_content = current_content + new_section
            commit_message = f"Auto-update: Added {len(papers)} new papers on {datetime.now(timezone.utc).strftime('%Y-%m-%d')}"
            
            self.repo.update_file(
                path="README.md",
                message=commit_message,
                content=updated_content,
                sha=readme_file.sha,
                branch="main"
            )
            
            logger.info(f"Successfully updated README with {len(papers)} papers")
            
        except Exception as e:
            logger.error(f"Error updating README: {e}")
            raise


def main():
    """Main function to run the paper fetcher."""
    import time
    
    start_time = time.time()
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒArXivè®ºæ–‡æŠ“å–ä»»åŠ¡")
    logger.info("=" * 60)
    
    # Get environment variables
    openai_api_key = os.getenv("OPENAI_API_KEY")
    github_token = os.getenv("TARGET_REPO_TOKEN")
    target_repo = os.getenv("TARGET_REPO_NAME", "YurenHao0426/awesome-llm-bias-papers")
    
    logger.info("ğŸ”§ é…ç½®ä¿¡æ¯:")
    logger.info(f"   - OpenAI API Key: {'å·²è®¾ç½®' if openai_api_key else 'æœªè®¾ç½®'}")
    logger.info(f"   - GitHub Token: {'å·²è®¾ç½®' if github_token else 'æœªè®¾ç½®'}")
    logger.info(f"   - ç›®æ ‡ä»“åº“: {target_repo}")
    
    # Check for required environment variables
    if not openai_api_key:
        logger.error("âŒ OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        sys.exit(1)
    
    if not github_token:
        logger.error("âŒ TARGET_REPO_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        sys.exit(1)
    
    # Get command line arguments
    mode = os.getenv("FETCH_MODE", "daily")  # daily or historical
    days = int(os.getenv("FETCH_DAYS", "1"))
    
    logger.info(f"ğŸ“‹ æ‰§è¡Œæ¨¡å¼: {mode}")
    if mode == "daily":
        logger.info(f"ğŸ“… æŠ“å–å¤©æ•°: {days} å¤©")
    
    try:
        step_start = time.time()
        
        # Initialize fetcher
        logger.info("ğŸ”„ åˆå§‹åŒ–è®ºæ–‡æŠ“å–å™¨...")
        fetcher = ArxivPaperFetcher(openai_api_key)
        logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆ ({time.time() - step_start:.1f}ç§’)")
        
        # Fetch papers
        step_start = time.time()
        if mode == "historical":
            logger.info("ğŸ“š è¿è¡Œå†å²æ¨¡å¼ - æŠ“å–è¿‡å»2å¹´çš„è®ºæ–‡")
            papers = fetcher.fetch_historical_papers(years=2)
            section_title = "Historical LLM Bias Papers (Past 2 Years)"
        else:
            logger.info(f"ğŸ“° è¿è¡Œæ—¥å¸¸æ¨¡å¼ - æŠ“å–è¿‡å»{days}å¤©çš„è®ºæ–‡")
            papers = fetcher.fetch_recent_papers(days=days)
            section_title = None  # Use default timestamp
        
        fetch_time = time.time() - step_start
        logger.info(f"â±ï¸ è®ºæ–‡æŠ“å–å’Œè¿‡æ»¤å®Œæˆ ({fetch_time:.1f}ç§’)")
        
        # Update GitHub repository
        if papers:
            step_start = time.time()
            logger.info(f"ğŸ“¤ å¼€å§‹æ›´æ–°GitHubä»“åº“...")
            updater = GitHubUpdater(github_token, target_repo)
            updater.update_readme_with_papers(papers, section_title)
            update_time = time.time() - step_start
            logger.info(f"âœ… GitHubä»“åº“æ›´æ–°å®Œæˆ ({update_time:.1f}ç§’)")
            
            logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆ!")
            logger.info(f"   - æ‰¾åˆ°ç›¸å…³è®ºæ–‡: {len(papers)} ç¯‡")
            logger.info(f"   - æ€»æ‰§è¡Œæ—¶é—´: {time.time() - start_time:.1f} ç§’")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            logger.info("å¯èƒ½çš„åŸå› :")
            logger.info("   - æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ–°çš„LLMåè§ç›¸å…³è®ºæ–‡")
            logger.info("   - arXiv APIè¿æ¥é—®é¢˜")
            logger.info("   - GPT-4oè¿‡æ»¤æ¡ä»¶è¿‡äºä¸¥æ ¼")
            logger.info(f"   - æ€»æ‰§è¡Œæ—¶é—´: {time.time() - start_time:.1f} ç§’")
            
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
