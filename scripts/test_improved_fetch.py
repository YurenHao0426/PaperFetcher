#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›åçš„è®ºæ–‡æŠ“å–åŠŸèƒ½

éªŒè¯åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«å’Œå»é‡é€»è¾‘æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
"""

import os
import sys
import logging
from datetime import datetime, timezone, timedelta

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Add the parent directory to the path so we can import the main module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.fetch_papers import ArxivPaperFetcher


def test_improved_fetching():
    """æµ‹è¯•æ”¹è¿›åçš„æŠ“å–é€»è¾‘"""
    
    print("ğŸš€ æµ‹è¯•æ”¹è¿›åçš„è®ºæ–‡æŠ“å–é€»è¾‘")
    print("=" * 60)
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„fetcherï¼ˆä¸éœ€è¦OpenAI APIï¼‰
    class MockArxivFetcher(ArxivPaperFetcher):
        def __init__(self):
            import requests
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'PaperFetcher/1.0 (Test)'
            })
    
    # æµ‹è¯•ä¸åŒçš„æ—¶é—´èŒƒå›´
    fetcher = MockArxivFetcher()
    
    print("\nğŸ• æµ‹è¯•1: è¿‡å»1å¤©ï¼ˆåº”è¯¥æ˜¾ç¤º0ç¯‡è®ºæ–‡ï¼‰")
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=1)
    papers_1day = fetcher.fetch_papers_by_date_range(start_date, end_date, max_papers=100)
    
    print(f"\nğŸ• æµ‹è¯•2: è¿‡å»7å¤©ï¼ˆåº”è¯¥æ˜¾ç¤ºæ›´å¤šè®ºæ–‡å’Œè¯¦ç»†åˆ†å¸ƒï¼‰")
    start_date = end_date - timedelta(days=7)
    papers_7days = fetcher.fetch_papers_by_date_range(start_date, end_date, max_papers=300)
    
    print(f"\nğŸ“Š æ”¹è¿›æ•ˆæœå¯¹æ¯”:")
    print(f"   - è¿‡å»1å¤©: {len(papers_1day)} ç¯‡è®ºæ–‡")
    print(f"   - è¿‡å»7å¤©: {len(papers_7days)} ç¯‡è®ºæ–‡")
    
    if papers_7days:
        print(f"\nğŸ“‹ è®ºæ–‡æ ·æœ¬ (å‰3ç¯‡):")
        for i, paper in enumerate(papers_7days[:3], 1):
            print(f"\n{i}. {paper['title'][:80]}...")
            print(f"   arXiv ID: {paper['arxiv_id']}")
            print(f"   æ›´æ–°æ—¶é—´: {paper['updated']}")
            print(f"   ç±»åˆ«: {', '.join(paper['categories'][:3])}")
            print(f"   ä½œè€…: {', '.join(paper['authors'][:2])}")
            if len(paper['authors']) > 2:
                print(f"         et al.")
    
    print(f"\nâœ… æ”¹è¿›åçš„ä¼˜åŠ¿:")
    print(f"   - âœ… åˆ†åˆ«æŸ¥è¯¢æ¯ä¸ªç±»åˆ«ï¼Œé¿å…ORæŸ¥è¯¢é™åˆ¶")
    print(f"   - âœ… è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤è®ºæ–‡")
    print(f"   - âœ… è¯¦ç»†çš„ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡")
    print(f"   - âœ… æ›´å‡†ç¡®çš„æ—¥æœŸåˆ†å¸ƒåˆ†æ")
    print(f"   - âœ… æ›´é€æ˜çš„æ—¥å¿—æ˜¾ç¤º")

def test_category_overlap():
    """æµ‹è¯•ç±»åˆ«é‡å å’Œå»é‡åŠŸèƒ½"""
    
    print(f"\n" + "="*60)
    print("ğŸ” æµ‹è¯•ç±»åˆ«é‡å å’Œå»é‡åŠŸèƒ½")
    print("="*60)
    
    # ç®€å•æµ‹è¯•ï¼šæ‰‹åŠ¨è·å–å‡ ä¸ªç±»åˆ«ï¼Œçœ‹çœ‹é‡å æƒ…å†µ
    import requests
    import feedparser
    from collections import defaultdict
    
    categories = ['cs.AI', 'cs.LG', 'cs.CL']
    papers_by_category = {}
    arxiv_ids_seen = set()
    overlaps = defaultdict(list)
    
    for cat in categories:
        print(f"\nğŸ“‚ è·å– {cat} ç±»åˆ«çš„è®ºæ–‡...")
        
        params = {
            'search_query': f'cat:{cat}',
            'sortBy': 'submittedDate',
            'sortOrder': 'descending',
            'max_results': 50
        }
        
        try:
            response = requests.get('http://export.arxiv.org/api/query', params=params, timeout=10)
            feed = feedparser.parse(response.content)
            entries = feed.entries
            
            papers_by_category[cat] = []
            
            for entry in entries:
                arxiv_id = entry.id.split('/')[-1]
                title = entry.title.replace('\n', ' ').strip()
                categories_list = [tag.term for tag in entry.tags] if hasattr(entry, 'tags') else []
                
                papers_by_category[cat].append({
                    'arxiv_id': arxiv_id,
                    'title': title,
                    'categories': categories_list
                })
                
                # æ£€æŸ¥é‡å 
                if arxiv_id in arxiv_ids_seen:
                    overlaps[arxiv_id].append(cat)
                else:
                    arxiv_ids_seen.add(arxiv_id)
                    overlaps[arxiv_id] = [cat]
            
            print(f"   è·å¾— {len(entries)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"   é”™è¯¯: {e}")
    
    # åˆ†æé‡å æƒ…å†µ
    print(f"\nğŸ“Š é‡å åˆ†æ:")
    total_papers = sum(len(papers) for papers in papers_by_category.values())
    unique_papers = len(arxiv_ids_seen)
    duplicate_papers = total_papers - unique_papers
    
    print(f"   - æ€»è·å–è®ºæ–‡: {total_papers} ç¯‡")
    print(f"   - å”¯ä¸€è®ºæ–‡: {unique_papers} ç¯‡")
    print(f"   - é‡å¤è®ºæ–‡: {duplicate_papers} ç¯‡")
    print(f"   - å»é‡ç‡: {duplicate_papers/total_papers*100:.1f}%")
    
    # æ˜¾ç¤ºä¸€äº›é‡å ä¾‹å­
    overlap_examples = [(arxiv_id, cats) for arxiv_id, cats in overlaps.items() if len(cats) > 1][:5]
    
    if overlap_examples:
        print(f"\nğŸ“‹ é‡å è®ºæ–‡ç¤ºä¾‹:")
        for arxiv_id, cats in overlap_examples:
            # æ‰¾åˆ°è¿™ç¯‡è®ºæ–‡çš„æ ‡é¢˜
            title = "æœªæ‰¾åˆ°æ ‡é¢˜"
            for cat, papers in papers_by_category.items():
                for paper in papers:
                    if paper['arxiv_id'] == arxiv_id:
                        title = paper['title'][:60] + "..." if len(paper['title']) > 60 else paper['title']
                        break
                if title != "æœªæ‰¾åˆ°æ ‡é¢˜":
                    break
            
            print(f"   - {arxiv_id}: {title}")
            print(f"     ç±»åˆ«: {', '.join(cats)}")
    
    print(f"\nâœ… è¿™è¯æ˜äº†å»é‡åŠŸèƒ½çš„é‡è¦æ€§ï¼")


if __name__ == "__main__":
    test_improved_fetching()
    test_category_overlap() 